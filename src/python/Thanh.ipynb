{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install numpy pandas nltk sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'../resources/IMDB Dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (len(df['review'])):\n",
    "    sentence = df['review'][i]\n",
    "    arr = sentence.split()\n",
    "    for word in arr: \n",
    "        if word == \"entertainedif\":\n",
    "            print(i, end = \" : \")\n",
    "            print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am writing this review having watched it several months ago....the trailer looked promising enough for me to buy this lame excuse for a movie. It is a complete joke....and literally a spit in the face of real classics of the early generation of horror like Texas Chainsaw Massacre (1974) which they even had the gall to compare itself to on the back of the cover art. The producer who played Brandon should go flip burgers and serve up greasy hamburgers....hell he might not even be good at that either! The lighting was bad bad bad and a big annoyance through out the film you couldn\\'t even see the actor\\'s faces sometimes. I don\\'t even remember the rest of the cast members which is sad really, bad they never do anything to impress you to make them memorable. That\\'s all the time I will waste on this review PLEASE stay as far away as you can from this pile of junk even if you get it for 25 cents don\\'t do it buy s piece of gum at least IT would keep you entertained!<br /><br />If you want good quality low budget fun, far better than this... then check out a Jeff Hayes film....because it takes talent to make it in horror and the kid has it!<br /><br />I gave this 1 star just for the cover art....thats the only thing worth liking abut this so called \"film\"<br /><br />-Rick Blalock'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][16123]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16123\n",
    "12853"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiền xử lý dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Làm sạch dữ liệu \n",
    "- Loại bỏ các thẻ HTML\n",
    "- Loại bỏ khoảng trắng thừa và dấu câu \n",
    "- Chuyển đổi chữ hoa thành chữ thường "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loại bỏ thẻ HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. The plot is simplistic, but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). While some may be disappointed when they realize this is not Match Point 2: Risk Addiction, I thought it was proof that Woody Allen is still fully in control of the style many of us have grown to love.This was the most I\\'d laughed at one of Woody\\'s comedies in years (dare I say a decade?). While I\\'ve never been impressed with Scarlet Johanson, in this she managed to tone down her \"sexy\" image and jumped right into a average, but spirited young woman.This may not be the crown jewel of his career, but it was wittier than \"Devil Wears Prada\" and more interesting than \"Superman\" a great comedy to go see with friends.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'] = df['review'].str.replace(\"<br />\", \"\")\n",
    "df['review'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loại bỏ khoảng trắng thừa và dấu câu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm để chuyển các ký tự đặc biệt thành khoảng trắng và loại bỏ khoảng trắng thừa\n",
    "def remove_punctuation(text):\n",
    "    # Chuyển các ký tự đặc biệt thành khoảng trắng\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    # Loại bỏ các khoảng trắng thừa\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am writing this review having watched it several months ago the trailer looked promising enough for me to buy this lame excuse for a movie It is a complete joke and literally a spit in the face of real classics of the early generation of horror like Texas Chainsaw Massacre 1974 which they even had the gall to compare itself to on the back of the cover art The producer who played Brandon should go flip burgers and serve up greasy hamburgers hell he might not even be good at that either The lighting was bad bad bad and a big annoyance through out the film you couldn t even see the actor s faces sometimes I don t even remember the rest of the cast members which is sad really bad they never do anything to impress you to make them memorable That s all the time I will waste on this review PLEASE stay as far away as you can from this pile of junk even if you get it for 25 cents don t do it buy s piece of gum at least IT would keep you entertained If you want good quality low budget fun far better than this then check out a Jeff Hayes film because it takes talent to make it in horror and the kid has it I gave this 1 star just for the cover art thats the only thing worth liking abut this so called film Rick Blalock'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][16123]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loại bỏ stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\PAVT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Tải stop words từ nltk\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words.remove('no')\n",
    "stop_words.remove('not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(text):\n",
    "    words = text.split()\n",
    "    filter_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filter_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['review'] = df['review'].apply(remove_stop_words)\n",
    "# df['review'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chuyển chữ hoa thành chữ thường "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i am writing this review having watched it several months ago the trailer looked promising enough for me to buy this lame excuse for a movie it is a complete joke and literally a spit in the face of real classics of the early generation of horror like texas chainsaw massacre 1974 which they even had the gall to compare itself to on the back of the cover art the producer who played brandon should go flip burgers and serve up greasy hamburgers hell he might not even be good at that either the lighting was bad bad bad and a big annoyance through out the film you couldn t even see the actor s faces sometimes i don t even remember the rest of the cast members which is sad really bad they never do anything to impress you to make them memorable that s all the time i will waste on this review please stay as far away as you can from this pile of junk even if you get it for 25 cents don t do it buy s piece of gum at least it would keep you entertained if you want good quality low budget fun far better than this then check out a jeff hayes film because it takes talent to make it in horror and the kid has it i gave this 1 star just for the cover art thats the only thing worth liking abut this so called film rick blalock'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'] = df['review'].str.lower()\n",
    "df['review'][16123]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chia dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_removed = df['review'].apply(remove_stop_words)\n",
    "# Chia dữ liệu thành tập huấn luyện và tập kiểm tra\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_removed, df['sentiment'], test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for sentence in X_train:\n",
    "    sentences.append(sentence.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers = 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_weights(vectors, w):\n",
    "    res = np.zeros(100)\n",
    "    for i in range(len(vectors)):\n",
    "        res += w[i]*vectors[i]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Các tiêu chí đánh giá phim: \n",
    "- Kịch bản: \n",
    "    + coherent/ incoherent\n",
    "    + unpredictable/ predictable\n",
    "- Ý nghĩa phim: meaningful/ meaningless\n",
    "- Hiệu ứng: impressive / unimpressive\n",
    "- Cảnh quay: heartfelt / insincere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_ReLU_prob(x):\n",
    "    if x < np.sqrt(3)/2:\n",
    "        return 0.4*x\n",
    "    else:\n",
    "        return 2*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_v(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_positive_words = ['heartfelt', 'gripping', 'impressive', 'meaningful', 'coherent']\n",
    "list_negative_words = ['insincere', 'predictable', 'soporific', 'illogical', 'uninteresting']\n",
    "# incoherent == uninteresting\n",
    "# meaningless == illogical\n",
    "# unimpressive == soporific\n",
    "# unpredictable == gripping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chưa cần dùng tới\n",
    "def find_most_dissimilar_words(model, word, top_n=10):\n",
    "    if word not in model:\n",
    "        raise ValueError(f\"Word '{word}' not in the model vocabulary.\")\n",
    "    \n",
    "    # Lấy vector của từ cần tìm\n",
    "    word_vector = model[word]\n",
    "    \n",
    "    # Tính khoảng cách cosine giữa từ cần tìm và tất cả các từ khác trong mô hình\n",
    "    distances = []\n",
    "    for other_word in model.index_to_key:\n",
    "        if other_word != word:\n",
    "            other_word_vector = model[other_word]\n",
    "            cosine_distance = np.dot(word_vector, other_word_vector) / (np.linalg.norm(word_vector) * np.linalg.norm(other_word_vector))\n",
    "            distances.append((other_word, cosine_distance))\n",
    "    \n",
    "    # Sắp xếp các từ theo khoảng cách cosine tăng dần\n",
    "    distances.sort(key=lambda x: x[1])\n",
    "    \n",
    "    # Lấy top_n từ có khoảng cách cosine lớn nhất (nghĩa khác xa nhất)\n",
    "    most_dissimilar_words = distances[:top_n]\n",
    "    \n",
    "    return most_dissimilar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Những từ có nghĩa khác xa nhất với 'uninteresting':\n",
      "appleseed: -0.4673237204551697\n",
      "wolstencroft: -0.4455452561378479\n",
      "saleslady: -0.4338872730731964\n",
      "burlesks: -0.4295748770236969\n",
      "photog: -0.4244859218597412\n",
      "siddons: -0.41785940527915955\n",
      "felson: -0.4168294072151184\n",
      "kirron: -0.4103279411792755\n",
      "lindsley: -0.4011898636817932\n",
      "earls: -0.39931991696357727\n"
     ]
    }
   ],
   "source": [
    "# Ví dụ: Tìm những từ có nghĩa khác xa nhất\n",
    "word = 'uninteresting'\n",
    "most_dissimilar_words = find_most_dissimilar_words(model.wv, word, top_n=10)\n",
    "\n",
    "print(f\"Những từ có nghĩa khác xa nhất với '{word}':\")\n",
    "for other_word, distance in most_dissimilar_words:\n",
    "    print(f\"{other_word}: {distance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- extraordinary/vivid >< dumbsh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_positive = [model.wv[word] for word in list_positive_words ]\n",
    "list_negative = [model.wv[word] for word in list_negative_words ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6893846\n",
      "0.546904\n",
      "0.56399274\n",
      "0.682836\n",
      "0.69089097\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(list_positive)):\n",
    "    print(cosine_similarity(list_positive[i], list_negative[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_vector = sum_weights(list_positive, [1, 2, 3, 1, 1])\n",
    "negative_vector = sum_weights(list_negative, [1, 2, 3, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5466110229191588"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(positive_vector, negative_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_vector(sentence, model, f = id_v):\n",
    "    word_vectors = [model.wv[word] for word in sentence if word in model.wv] # vector 100 từ\n",
    "    #dedicates= []\n",
    "    pos_weights = []\n",
    "    neg_weights = []\n",
    "    res = np.zeros(model.vector_size)\n",
    "    if len(word_vectors) == 0:\n",
    "        return res\n",
    "    for i in range(len(word_vectors)):\n",
    "        word_vector = word_vectors[i]\n",
    "        checkPositive = cosine_similarity(word_vector, positive_vector) # < 1\n",
    "        checkNegative = cosine_similarity(word_vector, negative_vector) # < 1\n",
    "        pos_weights.append(f(checkPositive))\n",
    "        neg_weights.append(f(checkNegative))\n",
    "\n",
    "    pos_res = sum_weights(word_vectors, pos_weights)\n",
    "    neg_res = sum_weights(word_vectors, neg_weights)\n",
    "    return np.concatenate([pos_res, neg_res])\n",
    "\n",
    "    # if(len(dedicates) > 0):\n",
    "    #     res = sum_weights(word_vectors,dedicates)\n",
    "    # return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tạo tập train và test cho model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_w2v_train = [ sentence_to_vector(sent.split(), model, id_v)  for sent in X_train]\n",
    "X_w2v_test =  [ sentence_to_vector(sent.split(), model, id_v)  for sent in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_removed = X_train.apply(remove_stop_words)\n",
    "X_test_removed = X_test.apply(remove_stop_words)\n",
    "X_w2v_train_removed = [ sentence_to_vector(sent.split(), model)  for sent in X_train_removed]\n",
    "X_w2v_test_removed =  [ sentence_to_vector(sent.split(), model)  for sent in X_test_removed]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mô hình\n",
    "chỉ chạy để kiểm thử model Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hàm khảo sát"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "def train_and_valid(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred_train = model.predict(X_train) \n",
    "    y_pred = model.predict(X_test)\n",
    "    # Đánh giá mô hình\n",
    "    accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "    accuracy_test = accuracy_score(y_test, y_pred)\n",
    "    print(f\"training accuracy: {accuracy_train} \\nvalidation accuracy: {accuracy_test}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.9797142857142858 \n",
      "validation accuracy: 0.7700666666666667\n"
     ]
    }
   ],
   "source": [
    "train_and_valid(clf, X_w2v_train, y_train, X_w2v_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_w2v_train_removed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_and_valid(clf, X_w2v_train_removed, y_train, X_w2v_test_removed, y_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_w2v_train_removed' is not defined"
     ]
    }
   ],
   "source": [
    "train_and_valid(clf, X_w2v_train_removed, y_train, X_w2v_test_removed, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Huấn luyện mô hình Logistic Regression\n",
    "log_regr = LogisticRegression(\n",
    "    random_state=42,\n",
    "    max_iter=400\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.8802857142857143 \n",
      "validation accuracy: 0.8788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "train_and_valid(log_regr,X_w2v_train, y_train, X_w2v_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.8616 \n",
      "validation accuracy: 0.8570666666666666\n"
     ]
    }
   ],
   "source": [
    "train_and_valid(log_regr,X_w2v_train_removed, y_train, X_w2v_test_removed, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import các thư viện cần thiết\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Huấn luyện mô hình Random Forest\n",
    "rf_clf = RandomForestClassifier(\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.9999714285714286 \n",
      "validation accuracy: 0.8488\n"
     ]
    }
   ],
   "source": [
    "train_and_valid(rf_clf, X_w2v_train, y_train, X_w2v_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.9999714285714286 \n",
      "validation accuracy: 0.8292666666666667\n"
     ]
    }
   ],
   "source": [
    "train_and_valid(rf_clf, X_w2v_train_removed, y_train, X_w2v_test_removed, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Tạo mô hình MLP\n",
    "mlp_classifier = MLPClassifier(\n",
    "    random_state=42,\n",
    "    hidden_layer_sizes=400,\n",
    "    max_iter=300\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.9736571428571429 \n",
      "validation accuracy: 0.8483333333333334\n"
     ]
    }
   ],
   "source": [
    "train_and_valid(mlp_classifier, X_w2v_train ,y_train ,X_w2v_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[0.5,1.5,0.75,0.75,1.5] --> 0.855 \\\n",
    "weights --> 0.8466"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tổng kết"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Mô hình Word Embeddings| Dữ liệu |Mô hình ML| Tham số | training accuracy | testing accuracy | Đánh giá | \n",
    "|---------|------|----------|------|--------|----|---|\n",
    "|Word2Vec|Không loại stop words |Decision tree | default | 0.979 | 0.7249 | Overfitting |\n",
    "|Word2Vec|Không loại stop words |Logistic Regression | max_iter = 400 | 0.852 | 0.850 | -- |\n",
    "|Word2Vec|Không loại stop words |Random forest | default | 0.99997 | 0.818 | Ovefitting |\n",
    "|Word2Vec|Không loại stop words  |XGBoost | default | 0.961 | 0.833 | Ovefitting |\n",
    "|Word2Vec|Không loại stop words |MLP | max_iter = 250, learning_rate_init = 0.0005 | 0.868 | 0.850 | -- |\n",
    "|Word2Vec|Loại stop words |Decision tree | default | 0.976 | 0.751 | Overfitting |\n",
    "|Word2Vec|Loại stop words |Logistic Regression | default | 0.850 | 0.848 | -- |\n",
    "|Word2Vec|Loại stop words |Random forest | default | 0.999 | 0.830 | Overfitting |\n",
    "|Word2Vec|Loại stop words  |XGBoost | default | 0.964 | 0.834 | Ovefitting |\n",
    "|Word2Vec|Loại stop words |MLP | default | 0.938 | 0.816 | Overfitting |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
